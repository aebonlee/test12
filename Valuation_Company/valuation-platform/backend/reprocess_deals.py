"""
Reprocess Deals
ê¸°ì¡´ì— ì €ì¥ëœ Deal ë°ì´í„° ì¬ì •ì œ (í’ˆì§ˆ í–¥ìƒ)
"""
import asyncio
import logging
import sys
import os
from dotenv import load_dotenv

# ê²½ë¡œ ì„¤ì •
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '.')))

from app.services.news_parser import NewsParser
from app.services.news_crawler.base_crawler import CrawledNews
from supabase import create_client

# ì„¤ì • ë¡œë“œ (ê°•ì œ ë¦¬ë¡œë“œ)
load_dotenv(override=True)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("ReprocessDeals")

supabase = create_client(os.getenv("SUPABASE_URL"), os.getenv("SUPABASE_KEY"))

async def reprocess_deals():
    logger.info("ğŸš€ ê¸°ì¡´ Deal ë°ì´í„° ì¬ì •ì œ ì‹œì‘ (IT/AI êµ¬ì²´í™”, ê¸ˆì•¡ ê²€ì¦)")
    
    # 1. ì¬ì •ì œ ëŒ€ìƒ ê°€ì ¸ì˜¤ê¸° (ì—…ì¢…ì´ IT, AI ì´ê±°ë‚˜ ê¸ˆì•¡ì´ ì—†ëŠ” ê²ƒ)
    # ì „ì²´ë¥¼ ë‹¤ì‹œ í•˜ëŠ” ê²ƒì´ ê°€ì¥ í™•ì‹¤í•¨
    response = supabase.table("deals").select("*").execute()
    deals = response.data
    
    logger.info(f"ì´ {len(deals)}ê°œì˜ Deal ë°ì´í„°ë¥¼ ì¬ê²€í† í•©ë‹ˆë‹¤.")
    
    parser = NewsParser()
    updated_count = 0
    
    for deal in deals:
        try:
            # ê¸°ì‚¬ ë‚´ìš©ì´ DBì— ì—†ìœ¼ë¯€ë¡œ, ì œëª©ê³¼ ìš”ì•½ ë“±ì„ ì´ìš©í•´ ë‹¤ì‹œ íŒŒì‹± ì‹œë„
            # (ì›ë˜ëŠ” URLë¡œ ë‹¤ì‹œ í¬ë¡¤ë§í•´ì•¼ í•˜ì§€ë§Œ, ì‹œê°„ìƒ ì œëª©+ê¸°ì¡´ì •ë³´ë¡œ ì¬ì¶”ë¡  ì‹œë„)
            # ë” ì •í™•í•˜ê²Œ í•˜ë ¤ë©´ í¬ë¡¤ëŸ¬ë¥¼ ë‹¤ì‹œ ëŒë ¤ì•¼ í•¨. ì—¬ê¸°ì„œëŠ” í¬ë¡¤ëŸ¬ë¥¼ ë‹¤ì‹œ ëŒë¦¬ëŠ” ë¡œì§ìœ¼ë¡œ êµ¬í˜„.
            
            logger.info(f"ğŸ”„ ì¬ì²˜ë¦¬ ì¤‘: {deal['company_name']} ({deal['news_title']})")
            
            # ì„ì‹œ ë‰´ìŠ¤ ê°ì²´ ìƒì„± (ì¬í¬ë¡¤ë§ ëŒ€ì‹  ì œëª©ë§Œìœ¼ë¡œ ì‹œë„í•´ë³´ê³ , ì•ˆë˜ë©´ í¬ë¡¤ëŸ¬ ì—°ë™ í•„ìš”)
            # ì¼ë‹¨ ì œëª©ì´ ê°€ì¥ ì¤‘ìš”í•˜ë¯€ë¡œ ì œëª© ê¸°ë°˜ìœ¼ë¡œ ì¬ì¶”ì¶œ ì‹œë„
            fake_news = CrawledNews(
                source=deal['site_name'] or "unknown",
                source_url=deal['news_url'] or "",
                title=deal['news_title'] or "",
                content=f"{deal['news_title']} {deal['company_name']} íˆ¬ì ìœ ì¹˜" # ë‚´ìš©ì´ ì—†ì–´ì„œ ì•½ì‹ ì²˜ë¦¬
            )
            
            # íŒŒì„œ í˜¸ì¶œ (ê°•í™”ëœ í”„ë¡¬í”„íŠ¸ ì ìš©ë¨)
            # ì£¼ì˜: ë‚´ìš©(content)ì´ ì—†ìœ¼ë©´ AIê°€ í•  ìˆ˜ ìˆëŠ”ê²Œ ì œí•œì ì„.
            # ì œëŒ€ë¡œ í•˜ë ¤ë©´ URLë¡œ ë‹¤ì‹œ ê¸ì–´ì•¼ í•¨.
            
            # ì´ë²ˆì—ëŠ” ê°„ë‹¨íˆ ì œëª©ë§Œìœ¼ë¡œë¼ë„ ì—…ì¢…ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì¶”ë¡ í•´ë‹¬ë¼ê³  ìš”ì²­
            extracted = await parser.parse_news(fake_news)
            
            if extracted and extracted.company_name_ko:
                update_data = {}
                
                # ì—…ì¢… êµ¬ì²´í™” í™•ì¸
                if extracted.industry and extracted.industry not in ["IT", "AI", "ì„œë¹„ìŠ¤"]:
                    update_data["industry"] = extracted.industry
                
                # ê¸ˆì•¡ ê²€ì¦ (ê¸°ì¡´ì— 0ì¸ë° ìƒˆë¡œ ì°¾ì•˜ìœ¼ë©´ ì—…ë°ì´íŠ¸)
                if extracted.investment_amount_krw and (not deal['amount'] or deal['amount'] == 0):
                    update_data["amount"] = extracted.investment_amount_krw
                
                # ì—…ë°ì´íŠ¸ ì‹¤í–‰
                if update_data:
                    supabase.table("deals").update(update_data).eq("id", deal['id']).execute()
                    logger.info(f"âœ… ì—…ë°ì´íŠ¸ ì™„ë£Œ: {deal['company_name']} -> {update_data}")
                    updated_count += 1
                else:
                    logger.info(f"PASS: ë³€ê²½ ì‚¬í•­ ì—†ìŒ ({deal['company_name']})")
            
        except Exception as e:
            logger.error(f"Error reprocessing deal {deal['id']}: {e}")

    logger.info(f"ğŸ‰ ì¬ì •ì œ ì™„ë£Œ: ì´ {updated_count}ê±´ ì—…ë°ì´íŠ¸ë¨")

if __name__ == "__main__":
    asyncio.run(reprocess_deals())
