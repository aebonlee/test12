"""
Daily Investment News Automation
ë§¤ì¼ ì•„ì¹¨ 6ì‹œ ì‹¤í–‰ë˜ëŠ” íˆ¬ì ë‰´ìŠ¤ ìë™í™” ì‹œìŠ¤í…œ

@process ë‰´ìŠ¤ ìˆ˜ì§‘ -> AI ì •ì œ -> DB ì €ì¥ -> ë¦¬í¬íŠ¸ ë°œì†¡
"""
import asyncio
import logging
import sys
import os
from datetime import datetime, timedelta

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '.')))

from app.services.news_crawler.crawler_manager import CrawlerManager
from app.services.news_parser import NewsParser
from app.services.investment_automation.enricher import DataEnricher
from app.services.investment_automation.reporter import DailyReporter
from supabase import create_client
from dotenv import load_dotenv

# ì„¤ì • ë¡œë“œ
load_dotenv(override=True)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("DailyAutomation")

# Supabase í´ë¼ì´ì–¸íŠ¸
supabase = create_client(os.getenv("SUPABASE_URL"), os.getenv("SUPABASE_KEY"))

async def run_daily_automation():
    logger.info("=" * 60)
    logger.info("ğŸš€ ë§¤ì¼ ì•„ì¹¨ íˆ¬ì ë‰´ìŠ¤ ìë™í™” í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
    logger.info("=" * 60)

    # 1. ë‰´ìŠ¤ ìˆ˜ì§‘ (ì „ë‚  ë‰´ìŠ¤ íƒ€ê²Ÿ)
    manager = CrawlerManager()
    keywords = ["íˆ¬ì ìœ ì¹˜", "ìŠ¤íƒ€íŠ¸ì—… íˆ¬ì", "ì‹œë¦¬ì¦ˆA", "ì‹œë“œ íˆ¬ì"]
    
    logger.info("1ï¸âƒ£ ë‰´ìŠ¤ ìˆ˜ì§‘ ë‹¨ê³„ ì‹œì‘...")
    yesterday_obj = datetime.now() - timedelta(days=1)
    yesterday = yesterday_obj.strftime("%Y-%m-%d")
    
    news_list = await manager.crawl_all(keywords=keywords, max_pages=3)
    logger.info(f"âœ… ì´ {len(news_list)}ê±´ì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")

    if not news_list:
        logger.info("ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # 2. AI ë°ì´í„° ì •ì œ ë° DB ì €ì¥
    parser = NewsParser()
    enricher = DataEnricher()
    logger.info("2ï¸âƒ£ AI ë°ì´í„° ì •ì œ ë° DB ì €ì¥ ë‹¨ê³„ ì‹œì‘...")
    logger.info(f"Connecting to Supabase: {os.getenv('SUPABASE_URL')}") # URL í™•ì¸ìš© ë¡œê·¸
    
    new_deals = [] # ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    saved_count = 0
    skipped_count = 0
    
    for news in news_list:
        try:
            # 1ì°¨ ì¤‘ë³µ ì²´í¬ (URL ê¸°ì¤€)
            existing_url = supabase.table("deals").select("id").eq("news_url", news.source_url).execute()
            if existing_url.data:
                logger.info(f"â­ï¸ [SKIP] ì´ë¯¸ ë“±ë¡ëœ ë‰´ìŠ¤ URL: {news.source_url}")
                skipped_count += 1
                continue

            # Gemini AIë¡œ ë°ì´í„° ì¶”ì¶œ
            extracted = await parser.parse_news(news)
            if not extracted or not extracted.company_name_ko:
                continue

            # 2ì°¨ ì¤‘ë³µ ì²´í¬ (ê¸°ì—…ëª… + íˆ¬ìê¸ˆì•¡ + íˆ¬ìë‹¨ê³„ ì •ë°€ ë¹„êµ)
            # ê°™ì€ ê¸°ì—…ì´ 'ê°™ì€ ë‹¨ê³„'ì—ì„œ 'ë¹„ìŠ·í•œ ê¸ˆì•¡'ì„ ë°›ì•˜ë‹¤ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
            # (ë‹¨ê³„ê°€ ë‹¤ë¥´ê±°ë‚˜ ê¸ˆì•¡ì´ ë‹¤ë¥´ë©´ ì¶”ê°€/í›„ì† íˆ¬ìë¡œ ì¸ì •)
            recent_deals = supabase.table("deals").select("id, amount, stage")\
                .eq("company_name", extracted.company_name_ko)\
                .gte("created_at", (datetime.now() - timedelta(days=60)).isoformat())\
                .execute()
            
            is_duplicate = False
            if recent_deals.data:
                for deal in recent_deals.data:
                    db_amount = deal.get('amount') or 0
                    new_amount = extracted.investment_amount_krw or 0
                    db_stage = deal.get('stage') or "unknown"
                    new_stage = extracted.investment_stage or "unknown"

                    # 1. ë‹¨ê³„ê°€ ê°™ê³ 
                    # 2. ê¸ˆì•¡ì´ ì—†ê±°ë‚˜(ë¹„ê³µê°œ), ê¸ˆì•¡ì´ ê±°ì˜ ì¼ì¹˜í•˜ë©´(ì˜¤ì°¨ 10% ì´ë‚´) -> ì¤‘ë³µ
                    if db_stage == new_stage:
                        if db_amount == 0 or new_amount == 0: # ê¸ˆì•¡ ë¶ˆëª…í™• ì‹œ ë‹¨ê³„ë§Œìœ¼ë¡œ ì¤‘ë³µ ì˜ì‹¬
                             is_duplicate = True
                             break
                        
                        # ê¸ˆì•¡ ë¹„êµ (10% ì˜¤ì°¨ í—ˆìš©)
                        if abs(db_amount - new_amount) < (db_amount * 0.1):
                            is_duplicate = True
                            break
            
            if is_duplicate:
                logger.info(f"â­ï¸ [SKIP] ì´ë¯¸ ë“±ë¡ëœ íˆ¬ì ê±´ (ë™ì¼ ë‹¨ê³„/ê¸ˆì•¡): {extracted.company_name_ko}")
                skipped_count += 1
                continue

            # 3ï¸âƒ£ ë°ì´í„° ë³´ê°• (Enrichment)
            if not extracted.industry or extracted.industry in ["IT", "AI"]:
                logger.info(f"ğŸ” {extracted.company_name_ko} ì •ë³´ ë³´ê°• ì¤‘...")
                enriched_data = await enricher.enrich_company_info(extracted.company_name_ko)
                # ë³´ê°• ë¡œì§ì€ ì¶”í›„ ê³ ë„í™”

            # ë°ì´í„° êµ¬ì„±
            deal_data = {
                "company_name": extracted.company_name_ko,
                "industry": extracted.industry,
                "stage": extracted.investment_stage,
                "amount": extracted.investment_amount_krw,
                "investors": ", ".join([inv.get("name", "") for inv in extracted.investors]) if extracted.investors else extracted.lead_investor,
                "news_title": news.title,
                "news_url": news.source_url,
                "news_date": yesterday,
                "site_name": news.source
            }

            # DB ì €ì¥
            supabase.table("deals").insert(deal_data).execute()
            new_deals.append(deal_data)
            logger.info(f"âœ… [ì €ì¥ ì™„ë£Œ] {extracted.company_name_ko} ({extracted.investment_amount_krw}ì–µì›)")

        except Exception as e:
            logger.error(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({news.source_url}): {e}")

    logger.info(f"ğŸ“Š ìµœì¢… ê²°ê³¼: {len(new_deals)}ê±´ì˜ ìƒˆë¡œìš´ Deal ë“±ë¡ ì™„ë£Œ")

    # 4. ë°ì¼ë¦¬ ë¦¬í¬íŠ¸ ë°œì†¡
    if new_deals:
        logger.info("4ï¸âƒ£ ë°ì¼ë¦¬ ë¦¬í¬íŠ¸ ë°œì†¡ ë‹¨ê³„ ì‹œì‘...")
        reporter = DailyReporter()
        target_emails = [os.getenv("REPORT_RECEIVER_EMAIL", os.getenv("SMTP_USER"))]
        reporter.send_report(new_deals, target_emails)

    logger.info("=" * 60)
    logger.info("ğŸ‰ ë§¤ì¼ ì•„ì¹¨ ìë™í™” í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ")
    logger.info("=" * 60)

if __name__ == "__main__":
    asyncio.run(run_daily_automation())
