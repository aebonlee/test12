#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë§¤ì¼ ìë™ ë‰´ìŠ¤ ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬ (ì™„ë²½í•œ í†µí•© ë²„ì „ v3)

í”„ë¡œì„¸ìŠ¤:
1. 5ëŒ€ ì–¸ë¡ ê¸°ê´€ ì›¹ í¬ë¡¤ë§
2. Google Searchë¡œ ì¶”ê°€ ìˆ˜ì§‘ (Gemini)
3. Geminië¡œ íˆ¬ì ë‰´ìŠ¤ ê²€ì¦
4. investment_news_articles í…Œì´ë¸” ì €ì¥
5. Deal í…Œì´ë¸” ë“±ë¡ (íšŒì‚¬ë‹¹ ìµœê³  ì ìˆ˜ 1ê°œ)
6. ëˆ„ë½ ì •ë³´ ì±„ìš°ê¸° (Geminië¡œ íˆ¬ìì, ì£¼ìš”ì‚¬ì—…)
7. ë„¤ì´ë²„ APIë¡œ ì¶”ê°€ ê²€ì¦/ë³´ì™„
8. ë„¤ì´ë²„ ë‰´ìŠ¤ â†’ ì‹¤ì œ ì–¸ë¡ ì‚¬ ë³€í™˜
9. Deal ë²ˆí˜¸ ì¬ì •ë ¬
10. ì´ë©”ì¼ ë°œì†¡

ì‹¤í–‰: python daily_auto_collect.py [--date YYYY-MM-DD]
"""

import os
import sys
import argparse
from datetime import datetime, timedelta, timezone
from dotenv import load_dotenv
from supabase import create_client
import requests
from bs4 import BeautifulSoup
import codecs
from google import genai
from google.genai import types
import time
import json
import re
from urllib.parse import urlparse, quote

if sys.platform == 'win32':
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')

load_dotenv()


# ì—…ì¢… ëŒ€ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ ë§¤í•‘
INDUSTRY_CATEGORIES = {
    'AI': ['AI', 'ì¸ê³µì§€ëŠ¥', 'AI ê¸°ë°˜', 'AI ì—ì´ì „íŠ¸', 'AIÂ·ì–‘ì', 'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹', 'LLM', 'ìƒì„±í˜•'],
    'í—¬ìŠ¤ì¼€ì–´': ['í—¬ìŠ¤ì¼€ì–´', 'ë°”ì´ì˜¤', 'ì˜ë£Œ', 'ì œì•½', 'ê±´ê°•', 'ì²´ì„±ë¶„', 'í´ë¦¬ë‹‰', 'ë©”ë””', 'ì§„ë‹¨', 'ì‹ ì•½', 'í—¬ìŠ¤'],
    'í•€í…Œí¬': ['í•€í…Œí¬', 'ê¸ˆìœµ', 'ê³µê¸‰ë§ ê¸ˆìœµ', 'ìì‚°', 'ë³´í—˜', 'í˜ì´', 'ê²°ì œ', 'ì¦ê¶Œ', 'ì¸ìŠˆì–´'],
    'ì´ì»¤ë¨¸ìŠ¤': ['ì´ì»¤ë¨¸ìŠ¤', 'ì»¤ë¨¸ìŠ¤', 'M&A í”Œë«í¼', 'ì‡¼í•‘', 'ë¦¬í…Œì¼', 'ìœ í†µ'],
    'ëª¨ë¹Œë¦¬í‹°': ['ëª¨ë¹Œë¦¬í‹°', 'ìë™ì°¨', 'ë¦¬ìŠ¤', 'ë ŒíŠ¸', 'ììœ¨ì£¼í–‰', 'ë¬¼ë¥˜', 'ë°°ì†¡'],
    'ë·°í‹°/íŒ¨ì…˜': ['ë·°í‹°', 'ìŠ¤í‚¨ì¼€ì–´', 'í™”ì¥í’ˆ', 'íŒ¨ì…˜', 'ì˜ë¥˜'],
    'ì½˜í…ì¸ /ì—”í„°': ['ì½˜í…ì¸ ', 'ì›¹íˆ°', 'IP ì œì‘', 'ì—”í„°', 'ê²Œì„', 'ë¯¸ë””ì–´', 'ì˜ìƒ'],
    'ìš°ì£¼í•­ê³µ': ['ìœ„ì„±', 'ìš°ì£¼', 'í•­ê³µ', 'ì—ì–´ë¡œìŠ¤í˜ì´ìŠ¤', 'ë“œë¡ '],
    'IT/í•˜ë“œì›¨ì–´': ['IT ê¸°ê¸°', 'í•˜ë“œì›¨ì–´', 'ë°˜ë„ì²´', 'ì„¼ì„œ', 'ë¡œë´‡', 'ì œì¡°'],
    'SaaS/B2B': ['SaaS', 'ì†”ë£¨ì…˜', 'í´ë¼ìš°ë“œ', 'B2B', 'HR', 'ERP'],
    'ë†ì—…/í‘¸ë“œ': ['ìŠ¤ë§ˆíŠ¸íŒœ', 'ë†ì—…', 'í‘¸ë“œ', 'ì‹í’ˆ', 'í‘¸ë“œí…Œí¬', 'ì—ì–´ë¡œí¬ë‹‰'],
    'ì—ë“€í…Œí¬': ['ì—ë“€', 'êµìœ¡', 'í•™ìŠµ', 'ëŸ¬ë‹'],
    'ë¶€ë™ì‚°/ê±´ì„¤': ['ë¶€ë™ì‚°', 'í”„ë¡­', 'ê±´ì„¤', 'ê±´ì¶•', 'ì¸í…Œë¦¬ì–´'],
    'ì—ë„ˆì§€/í™˜ê²½': ['ì—ë„ˆì§€', 'íƒœì–‘ê´‘', 'ë°°í„°ë¦¬', 'íƒ„ì†Œ', 'í™˜ê²½', 'ê·¸ë¦°', 'ìˆ˜ì†Œ', 'ì¹œí™˜ê²½'],
}


def categorize_industry(raw_industry):
    """ì„¸ë¶€ ì—…ì¢…ì„ ëŒ€ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ë¡œ ë§¤í•‘"""
    if not raw_industry:
        return None
    for category, keywords in INDUSTRY_CATEGORIES.items():
        for kw in keywords:
            if kw in raw_industry:
                return category
    return 'ê¸°íƒ€'


# íˆ¬ìë‹¨ê³„ ì •ê·œí™” ë§¤í•‘
STAGE_MAP = {
    'ì‹œë¦¬ì¦ˆ A': 'ì‹œë¦¬ì¦ˆA', 'ì‹œë¦¬ì¦ˆ B': 'ì‹œë¦¬ì¦ˆB', 'ì‹œë¦¬ì¦ˆ C': 'ì‹œë¦¬ì¦ˆC',
    'ì‹œë¦¬ì¦ˆ D': 'ì‹œë¦¬ì¦ˆD', 'ì‹œë¦¬ì¦ˆ E': 'ì‹œë¦¬ì¦ˆE',
    'í”„ë¦¬ A': 'í”„ë¦¬A', 'í”„ë¦¬ ì‹œë“œ': 'í”„ë¦¬ì‹œë“œ',
    'Series A': 'ì‹œë¦¬ì¦ˆA', 'Series B': 'ì‹œë¦¬ì¦ˆB', 'Series C': 'ì‹œë¦¬ì¦ˆC',
    'Series D': 'ì‹œë¦¬ì¦ˆD', 'Series E': 'ì‹œë¦¬ì¦ˆE',
    'Pre-A': 'í”„ë¦¬A', 'Pre A': 'í”„ë¦¬A',
    'Seed': 'ì‹œë“œ', 'Pre-Seed': 'í”„ë¦¬ì‹œë“œ', 'Pre-seed': 'í”„ë¦¬ì‹œë“œ',
    'Bridge': 'ë¸Œë¦¿ì§€', 'Pre-IPO': 'í”„ë¦¬IPO', 'pre-IPO': 'í”„ë¦¬IPO',
}
VALID_STAGES = {'í”„ë¦¬ì‹œë“œ', 'ì‹œë“œ', 'í”„ë¦¬A', 'í”„ë¦¬A ë¸Œë¦¿ì§€', 'ì‹œë¦¬ì¦ˆA', 'ì‹œë¦¬ì¦ˆB',
                'ì‹œë¦¬ì¦ˆC', 'ì‹œë¦¬ì¦ˆD', 'ì‹œë¦¬ì¦ˆE', 'í”„ë¦¬IPO', 'M&A', 'ë¸Œë¦¿ì§€'}


def normalize_stage(raw_stage):
    """íˆ¬ìë‹¨ê³„ë¥¼ ì •ê·œí™” (ë„ì–´ì“°ê¸°/ì˜ì–´ í†µì¼, íˆ¬ììëª… ì˜¤ì…ë ¥ ì œê±°)"""
    if not raw_stage:
        return None
    stage = raw_stage.strip()
    # ë§¤í•‘ í…Œì´ë¸”ì— ìˆìœ¼ë©´ ë³€í™˜
    if stage in STAGE_MAP:
        return STAGE_MAP[stage]
    # ìœ íš¨í•œ stageë©´ ê·¸ëŒ€ë¡œ
    if stage in VALID_STAGES:
        return stage
    # ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ null (íˆ¬ììëª… ì˜¤ì…ë ¥ ë“±)
    return None


# Supabase & Gemini í´ë¼ì´ì–¸íŠ¸
supabase = create_client(os.getenv("SUPABASE_URL"), os.getenv("SUPABASE_SERVICE_KEY"))
gemini_client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))

# 5ëŒ€ ì–¸ë¡ ê¸°ê´€
MEDIA_SITES = [
    {
        'id': 9,
        'name': 'ë²¤ì²˜ìŠ¤í€˜ì–´',
        'url': 'https://www.venturesquare.net/category/news/',
        'article_selector': 'article.post',
        'title_selector': 'h5 a',
        'link_selector': 'h5 a',
    },
    {
        'id': 11,
        'name': 'ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´',
        'url': 'https://www.startuptoday.kr/news/articleList.html',
        'article_selector': 'div.article-list-content',
        'title_selector': 'h4.titles',
        'link_selector': 'a',
    },
    {
        'id': 13,
        'name': 'ì•„ì›ƒìŠ¤íƒ ë”©',
        'url': 'https://outstanding.kr/',
        'article_selector': 'article',
        'title_selector': 'h2 a',
        'link_selector': 'h2 a',
    },
    {
        'id': 10,
        'name': 'í”Œë˜í…€',
        'url': 'https://platum.kr/',
        'article_selector': 'article',
        'title_selector': 'h2 a',
        'link_selector': 'h2 a',
    },
    {
        'id': 1,
        'name': 'WOWTALE',
        'url': 'https://wowtale.net/',
        'article_selector': 'article',
        'title_selector': 'h2 a',
        'link_selector': 'h2 a',
    },
]


def log(message, level="INFO"):
    """ë¡œê·¸ ì¶œë ¥"""
    timestamp = datetime.now().strftime('%H:%M:%S')
    print(f"[{timestamp}] [{level}] {message}")


def extract_article_date(html_content, url):
    """ê¸°ì‚¬ HTMLì—ì„œ ë°œí–‰ì¼ ì¶”ì¶œ"""
    try:
        soup = BeautifulSoup(html_content, 'html.parser')

        # ë©”íƒ€ íƒœê·¸ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
        date_meta = soup.find('meta', {'property': 'article:published_time'})
        if date_meta:
            date_str = date_meta.get('content', '')
            return date_str.split('T')[0] if 'T' in date_str else date_str[:10]

        # time íƒœê·¸
        time_tag = soup.find('time')
        if time_tag:
            datetime_attr = time_tag.get('datetime')
            if datetime_attr:
                return datetime_attr.split('T')[0] if 'T' in datetime_attr else datetime_attr[:10]

        return None
    except:
        return None


def verify_with_gemini(title, url):
    """Geminië¡œ íˆ¬ì ë‰´ìŠ¤ì¸ì§€ ê²€ì¦"""
    prompt = f"""
ë‹¤ìŒ ë‰´ìŠ¤ ì œëª©ì´ ìŠ¤íƒ€íŠ¸ì—… íˆ¬ììœ ì¹˜ ë‰´ìŠ¤ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”:

ì œëª©: {title}

JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€:
{{
    "is_investment": true,
    "company": "íšŒì‚¬ëª…",
    "stage": "ì‹œë“œ/í”„ë¦¬A/ì‹œë¦¬ì¦ˆA ë“±",
    "investors": "íˆ¬ììëª…",
    "amount": ìˆ«ìë§Œ
}}

íˆ¬ììœ ì¹˜ ë‰´ìŠ¤ê°€ ì•„ë‹ˆë©´ {{"is_investment": false}}ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""

    try:
        response = gemini_client.models.generate_content(
            model='gemini-2.5-flash',
            contents=prompt,
            config=types.GenerateContentConfig(
                temperature=0,
                max_output_tokens=256,
                response_mime_type='application/json'
            )
        )

        if response and hasattr(response, 'text'):
            text = response.text.strip()
            result = json.loads(text)
            return result

        return None
    except Exception as e:
        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ íŒë‹¨
        invest_keywords = ['íˆ¬ì', 'ìœ ì¹˜', 'í€ë”©', 'ì‹œë¦¬ì¦ˆ', 'ë¼ìš´ë“œ']
        if any(kw in title for kw in invest_keywords):
            return {'is_investment': True, 'company': None, 'stage': None, 'investors': None, 'amount': None}
        return {'is_investment': False}


def extract_deal_info_with_gemini(title, url):
    """Geminië¡œ ë‰´ìŠ¤ì—ì„œ Deal ì •ë³´ ì¶”ì¶œ"""
    prompt = f"""
ë‹¤ìŒ íˆ¬ììœ ì¹˜ ë‰´ìŠ¤ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

ì œëª©: {title}

JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€:
{{
    "company_name": "íšŒì‚¬ëª… (ë²•ì¸ëª…)",
    "industry": "ì£¼ìš”ì‚¬ì—… (AI/í—¬ìŠ¤ì¼€ì–´/í•€í…Œí¬ ë“±)",
    "stage": "íˆ¬ìë‹¨ê³„ (ì‹œë“œ/í”„ë¦¬A/ì‹œë¦¬ì¦ˆA ë“±)",
    "investors": "íˆ¬ìì (ì½¤ë§ˆë¡œ êµ¬ë¶„)",
    "amount": "íˆ¬ìê¸ˆì•¡ (ì–µì› ìˆ«ìë§Œ)",
    "location": "ì§€ì—­",
    "employees": "ì§ì›ìˆ˜ (ìˆ«ìë§Œ)"
}}

ì¡°ê±´:
- ì •ë³´ ì—†ìœ¼ë©´ null
- amountëŠ” ì–µì› ë‹¨ìœ„ ìˆ«ìë§Œ (50ì–µ â†’ 50)
- employeesëŠ” ìˆ«ìë§Œ
- íˆ¬ììœ ì¹˜ ë‰´ìŠ¤ê°€ ì•„ë‹ˆë©´ company_nameì„ nullë¡œ
- âš ï¸ company_nameì€ ë°˜ë“œì‹œ ë²•ì¸ëª…/íšŒì‚¬ëª…ì´ì–´ì•¼ í•¨ (ì„œë¹„ìŠ¤ëª…/ë¸Œëœë“œëª…/í”Œë«í¼ëª… ê¸ˆì§€!)
  ì˜ˆ: "ì°¨ì¦˜ì„ ìš´ì˜í•˜ëŠ” ë””ìì¸ì•¤í”„ë™í‹°ìŠ¤" â†’ company_nameì€ "ë””ìì¸ì•¤í”„ë™í‹°ìŠ¤" (ì°¨ì¦˜ ì•„ë‹˜)
  ì˜ˆ: "í† ìŠ¤ë¥¼ ìš´ì˜í•˜ëŠ” ë¹„ë°”ë¦¬í¼ë¸”ë¦¬ì¹´" â†’ company_nameì€ "ë¹„ë°”ë¦¬í¼ë¸”ë¦¬ì¹´" (í† ìŠ¤ ì•„ë‹˜)
"""

    try:
        response = gemini_client.models.generate_content(
            model='gemini-2.5-flash',
            contents=prompt,
            config=types.GenerateContentConfig(
                temperature=0,
                max_output_tokens=512,
                response_mime_type='application/json'
            )
        )

        if response and hasattr(response, 'text'):
            text = response.text.strip()
            result = json.loads(text)
            return result

        return None
    except Exception as e:
        return None


def calculate_score(info):
    """ê¸°ì‚¬ ì ìˆ˜ ê³„ì‚° (11ì  ë§Œì )"""
    score = 0

    if info.get('amount'):
        score += 3
    if info.get('investors'):
        score += 3
    if info.get('stage'):
        score += 2
    if info.get('industry'):
        score += 1
    if info.get('location'):
        score += 1
    if info.get('employees'):
        score += 1

    return score


def extract_site_name_from_url(url):
    """URLì—ì„œ ì‹¤ì œ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ"""
    try:
        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')

            # og:site_name ë©”íƒ€ íƒœê·¸
            og_site = soup.find('meta', {'property': 'og:site_name'})
            if og_site and og_site.get('content'):
                return og_site.get('content').strip()

            # publisher ë©”íƒ€ íƒœê·¸
            publisher = soup.find('meta', {'name': 'publisher'})
            if publisher and publisher.get('content'):
                return publisher.get('content').strip()

        return None
    except:
        return None


# ============================================================
# Step 1: 5ëŒ€ ì–¸ë¡ ê¸°ê´€ ì›¹ í¬ë¡¤ë§
# ============================================================
def step1_crawl_media_sites(target_date):
    """5ëŒ€ ì–¸ë¡ ê¸°ê´€ì—ì„œ ë‰´ìŠ¤ í¬ë¡¤ë§"""
    log(f"Step 1: 5ëŒ€ ì–¸ë¡ ê¸°ê´€ í¬ë¡¤ë§ ì‹œì‘ (ëª©í‘œ ë‚ ì§œ: {target_date})")

    all_articles = []

    for site in MEDIA_SITES:
        log(f"  ğŸ“° {site['name']} í¬ë¡¤ë§ ì¤‘...")

        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
            response = requests.get(site['url'], headers=headers, timeout=10)

            if response.status_code != 200:
                log(f"    âŒ HTTP {response.status_code}", "ERROR")
                continue

            soup = BeautifulSoup(response.content, 'html.parser')
            article_elements = soup.select(site['article_selector'])[:20]

            site_articles = 0
            for article in article_elements:
                try:
                    title_elem = article.select_one(site['title_selector'])
                    link_elem = article.select_one(site['link_selector'])

                    if not title_elem or not link_elem:
                        continue

                    title = title_elem.get_text(strip=True)
                    url = link_elem.get('href', '')

                    # ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
                    if url.startswith('/'):
                        base_url = site['url'].split('?')[0].rsplit('/', 1)[0]
                        url = base_url + url

                    if not url.startswith('http'):
                        continue

                    # íˆ¬ì í‚¤ì›Œë“œ í•„í„°
                    invest_keywords = ['íˆ¬ì', 'ìœ ì¹˜', 'í€ë”©', 'ì‹œë¦¬ì¦ˆ', 'Series', 'ë¼ìš´ë“œ', 'Pre-A', 'ì‹œë“œ']
                    if any(kw in title for kw in invest_keywords):
                        all_articles.append({
                            'site_id': site['id'],
                            'site_name': site['name'],
                            'title': title,
                            'url': url,
                        })
                        site_articles += 1

                    time.sleep(0.3)
                except:
                    continue

            log(f"    âœ… {site_articles}ê°œ ë°œê²¬")

        except Exception as e:
            log(f"    âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)[:50]}", "ERROR")

        time.sleep(1)

    log(f"  ğŸ“Š ì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
    return all_articles


# ============================================================
# Step 1.5: Google Searchë¡œ ì¶”ê°€ ìˆ˜ì§‘ (Gemini Grounding)
# ============================================================
def step1_5_google_search(target_date, existing_urls):
    """Google Searchë¡œ ì¶”ê°€ íˆ¬ì ë‰´ìŠ¤ ìˆ˜ì§‘"""
    log(f"Step 1.5: Google Search ì¶”ê°€ ìˆ˜ì§‘")

    search_queries = [
        f"ìŠ¤íƒ€íŠ¸ì—… íˆ¬ììœ ì¹˜ {target_date}",
        f"ì‹œë¦¬ì¦ˆA íˆ¬ì {target_date}",
        f"ë²¤ì²˜íˆ¬ì ìœ ì¹˜ {target_date}",
        f"ìŠ¤íƒ€íŠ¸ì—… í€ë”© {target_date}",
    ]

    additional_articles = []

    for query in search_queries:
        log(f"  ğŸ” ê²€ìƒ‰: {query[:30]}...")

        prompt = f"""
ë‹¤ìŒ ê²€ìƒ‰ì–´ë¡œ í•œêµ­ ìŠ¤íƒ€íŠ¸ì—… íˆ¬ììœ ì¹˜ ë‰´ìŠ¤ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”:
"{query}"

ìµœê·¼ ë‰´ìŠ¤ ì¤‘ ì‹¤ì œ íˆ¬ììœ ì¹˜ ë°œí‘œ ë‰´ìŠ¤ë§Œ ì°¾ì•„ì„œ JSON ë°°ì—´ë¡œ ë‹µë³€í•˜ì„¸ìš”:
[
    {{
        "title": "ê¸°ì‚¬ ì œëª©",
        "url": "ê¸°ì‚¬ URL",
        "source": "ì–¸ë¡ ì‚¬ëª…"
    }}
]

ì¡°ê±´:
- ì‹¤ì œ íˆ¬ììœ ì¹˜ ë°œí‘œ ë‰´ìŠ¤ë§Œ (ë‹¨ìˆœ ë¶„ì„/ì „ë§ ê¸°ì‚¬ ì œì™¸)
- í•œêµ­ ìŠ¤íƒ€íŠ¸ì—… ê´€ë ¨ë§Œ
- ìµœëŒ€ 5ê°œê¹Œì§€
- ë‰´ìŠ¤ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´ []
"""

        try:
            response = gemini_client.models.generate_content(
                model='gemini-2.5-flash',
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0,
                    max_output_tokens=1024,
                    tools=[types.Tool(google_search=types.GoogleSearch())]
                )
            )

            if response and hasattr(response, 'text'):
                text = response.text.strip()

                # JSON ì¶”ì¶œ
                json_match = re.search(r'\[.*\]', text, re.DOTALL)
                if json_match:
                    results = json.loads(json_match.group())

                    for item in results:
                        url = item.get('url', '')

                        # ì¤‘ë³µ ì²´í¬
                        if url and url not in existing_urls:
                            additional_articles.append({
                                'site_id': 0,  # Google Search
                                'site_name': item.get('source', 'Google Search'),
                                'title': item.get('title', ''),
                                'url': url,
                            })
                            existing_urls.add(url)

                    log(f"    âœ… {len(results)}ê°œ ë°œê²¬")
                else:
                    log(f"    âš ï¸ ê²°ê³¼ ì—†ìŒ")

        except Exception as e:
            log(f"    âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)[:40]}", "ERROR")

        time.sleep(2)  # API ì œí•œ ê³ ë ¤

    log(f"  ğŸ“Š Google Searchë¡œ {len(additional_articles)}ê°œ ì¶”ê°€ ìˆ˜ì§‘")
    return additional_articles


# ============================================================
# Step 2: Gemini ê²€ì¦ + ì €ì¥
# ============================================================
def step2_verify_and_save(articles, target_date):
    """Geminië¡œ ê²€ì¦í•˜ê³  investment_news_articlesì— ì €ì¥"""
    log(f"Step 2: Gemini ê²€ì¦ ë° ì €ì¥")

    saved = 0

    for i, article in enumerate(articles, 1):
        log(f"  [{i}/{len(articles)}] {article['title'][:40]}...")

        # ì™€ìš°í…Œì¼ ê³µì§€ì‚¬í•­ ì œì™¸
        if '[ê³µì§€]' in article['title'] or 'ê³µì§€ì‚¬í•­' in article['title']:
            log(f"    âš ï¸ ê³µì§€ì‚¬í•­ ì œì™¸")
            continue

        # ì¤‘ë³µ ì²´í¬
        existing = supabase.table('investment_news_articles').select('id').eq('article_url', article['url']).execute()
        if existing.data:
            log(f"    âš ï¸ ì¤‘ë³µ")
            continue

        # ë‚ ì§œ ì¶”ì¶œ
        try:
            article_response = requests.get(article['url'], headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
            published_date = extract_article_date(article_response.content, article['url']) if article_response.status_code == 200 else None
        except:
            published_date = None

        # ë‚ ì§œ í•„í„°
        if published_date != target_date:
            log(f"    âŒ ë‚ ì§œ ë²”ìœ„ ë°– ({published_date})")
            continue

        # Gemini ê²€ì¦
        gemini_result = verify_with_gemini(article['title'], article['url'])

        if gemini_result and gemini_result.get('is_investment'):
            # ì €ì¥
            try:
                supabase.table('investment_news_articles').insert({
                    'site_number': article['site_id'],
                    'site_name': article['site_name'],
                    'site_url': urlparse(article['url']).netloc,
                    'article_title': article['title'],
                    'article_url': article['url'],
                    'published_date': published_date,
                    'collected_at': datetime.now().isoformat(),  # ìˆ˜ì§‘ ì‹œê°„ ì €ì¥
                    'has_amount': gemini_result.get('amount') is not None,
                    'has_investors': gemini_result.get('investors') is not None,
                    'has_stage': gemini_result.get('stage') is not None,
                }).execute()

                saved += 1
                log(f"    âœ… ì €ì¥ ì™„ë£Œ")
            except Exception as e:
                log(f"    âŒ ì €ì¥ ì˜¤ë¥˜: {str(e)[:40]}", "ERROR")
        else:
            log(f"    âŒ íˆ¬ì ë‰´ìŠ¤ ì•„ë‹˜")

        time.sleep(1)

    log(f"  ğŸ“Š {saved}ê°œ ì €ì¥ ì™„ë£Œ")
    return saved


# ============================================================
# Step 3: Deal í…Œì´ë¸” ë“±ë¡
# ============================================================
def step3_register_to_deals(target_date):
    """Deal í…Œì´ë¸”ì— ë“±ë¡ (íšŒì‚¬ë‹¹ ìµœê³  ì ìˆ˜ 1ê°œ)"""
    log(f"Step 3: Deal í…Œì´ë¸” ë“±ë¡")

    # í•´ë‹¹ ë‚ ì§œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    articles = supabase.table('investment_news_articles').select('*').eq('published_date', target_date).execute()

    if not articles.data:
        log(f"  âš ï¸ í•´ë‹¹ ë‚ ì§œ ë‰´ìŠ¤ ì—†ìŒ")
        return 0

    log(f"  ğŸ“° {len(articles.data)}ê°œ ë‰´ìŠ¤ ì²˜ë¦¬ ì¤‘...")

    # ê° ë‰´ìŠ¤ì—ì„œ ì •ë³´ ì¶”ì¶œ
    news_with_info = []

    for article in articles.data:
        info = extract_deal_info_with_gemini(article['article_title'], article['article_url'])

        if info and info.get('company_name'):
            score = calculate_score(info)
            news_with_info.append({
                'article': article,
                'info': info,
                'score': score
            })
            log(f"    âœ… {info['company_name']} (ì ìˆ˜: {score})")

        time.sleep(0.8)

    log(f"  ğŸ“Š {len(news_with_info)}ê°œ íšŒì‚¬ ë°œê²¬")

    # íšŒì‚¬ë³„ ìµœê³  ì ìˆ˜ ì„ íƒ
    company_best = {}
    for news in news_with_info:
        company = news['info']['company_name']
        score = news['score']

        if company not in company_best or score > company_best[company]['score']:
            company_best[company] = news

    # ê¸°ì¡´ deals ê°€ì ¸ì˜¤ê¸° (ì ìˆ˜ ë¹„êµìš©)
    existing_deals = supabase.table('deals').select('*').execute()

    # íšŒì‚¬ë³„ ê¸°ì¡´ deal ì •ë³´ ì €ì¥ {íšŒì‚¬ëª…: {stage: deal_data}}
    existing_company_deals = {}
    existing_news_urls = set()

    for deal in existing_deals.data:
        company = deal['company_name']
        stage = deal.get('stage') or 'unknown'
        news_url = deal.get('news_url')

        if company not in existing_company_deals:
            existing_company_deals[company] = {}

        # ê¸°ì¡´ dealì˜ ì ìˆ˜ ê³„ì‚°
        existing_score = 0
        if deal.get('amount'): existing_score += 3
        if deal.get('investors'): existing_score += 3
        if deal.get('stage'): existing_score += 2
        if deal.get('industry'): existing_score += 1
        if deal.get('location'): existing_score += 1

        existing_company_deals[company][stage] = {
            'id': deal['id'],
            'score': existing_score,
            'deal': deal
        }

        if news_url:
            existing_news_urls.add(news_url)

    last_deal = supabase.table('deals').select('number').order('number', desc=True).limit(1).execute()
    next_number = last_deal.data[0]['number'] + 1 if last_deal.data else 1

    registered = 0
    updated = 0

    for company, news in company_best.items():
        article = news['article']
        info = news['info']
        new_stage = normalize_stage(info.get('stage')) or 'unknown'
        new_score = news['score']
        news_url = article.get('article_url')

        # 1. ê°™ì€ ë‰´ìŠ¤ URLì´ë©´ ì¤‘ë³µ
        if news_url in existing_news_urls:
            log(f"    âš ï¸ {company}: ê°™ì€ ë‰´ìŠ¤ URL ì¡´ì¬")
            continue

        # 2. íšŒì‚¬ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if company in existing_company_deals:
            company_stages = existing_company_deals[company]

            # ê°™ì€ ë¼ìš´ë“œê°€ ìˆëŠ”ì§€ í™•ì¸
            if new_stage in company_stages:
                existing_info = company_stages[new_stage]
                existing_score = existing_info['score']

                # ì ìˆ˜ ë¹„êµ: ìƒˆ ë‰´ìŠ¤ê°€ ë” ë†’ìœ¼ë©´ ì—…ë°ì´íŠ¸
                if new_score > existing_score:
                    log(f"    ğŸ”„ {company}: ë” ë†’ì€ ì ìˆ˜ ë‰´ìŠ¤ ë°œê²¬ ({existing_score} â†’ {new_score})")

                    # ê¸°ì¡´ deal ì—…ë°ì´íŠ¸
                    new_industry = info.get('industry') or existing_info['deal'].get('industry')
                    supabase.table('deals').update({
                        'industry': new_industry,
                        'industry_category': categorize_industry(new_industry),
                        'investors': info.get('investors') or existing_info['deal'].get('investors'),
                        'amount': info.get('amount') or existing_info['deal'].get('amount'),
                        'location': info.get('location') or existing_info['deal'].get('location'),
                        'news_title': article['article_title'],
                        'news_url': article['article_url'],
                        'news_date': article['published_date'],
                        'site_name': article['site_name'],
                    }).eq('id', existing_info['id']).execute()

                    updated += 1
                    log(f"    âœ… {company} ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                else:
                    log(f"    âš ï¸ {company}: ê°™ì€ ë¼ìš´ë“œ({new_stage}) ì´ë¯¸ ì¡´ì¬ (ê¸°ì¡´ ì ìˆ˜ {existing_score} >= ìƒˆ ì ìˆ˜ {new_score})")
                continue

            else:
                # ìƒˆë¡œìš´ íˆ¬ì ë¼ìš´ë“œ
                log(f"    ğŸ†• {company}: ìƒˆë¡œìš´ íˆ¬ì ë¼ìš´ë“œ({new_stage}) ë°œê²¬!")

        # ì‹ ê·œ ë“±ë¡
        try:
            supabase.table('deals').insert({
                'number': next_number,
                'company_name': company,
                'industry': info.get('industry'),
                'industry_category': categorize_industry(info.get('industry')),
                'stage': normalize_stage(info.get('stage')),
                'investors': info.get('investors'),
                'amount': info.get('amount'),
                'location': info.get('location'),
                'news_title': article['article_title'],
                'news_url': article['article_url'],
                'news_date': article['published_date'],
                'site_name': article['site_name'],
            }).execute()

            log(f"    âœ… {company} ë“±ë¡ (#{next_number})")

            # ë“±ë¡ëœ íšŒì‚¬ì˜ stage ì •ë³´ ì—…ë°ì´íŠ¸
            if company not in existing_company_deals:
                existing_company_deals[company] = {}
            existing_company_deals[company][new_stage] = {'score': new_score}
            existing_news_urls.add(news_url)

            next_number += 1
            registered += 1

        except Exception as e:
            log(f"    âŒ {company} ì˜¤ë¥˜: {str(e)[:40]}", "ERROR")

    log(f"  ğŸ“Š ì‹ ê·œ {registered}ê°œ ë“±ë¡, {updated}ê°œ ì—…ë°ì´íŠ¸")
    return registered + updated


# ============================================================
# Step 4: ëˆ„ë½ ì •ë³´ ì±„ìš°ê¸° (Geminië¡œ ë‰´ìŠ¤ ë³¸ë¬¸ì—ì„œ ì¶”ì¶œ)
# ============================================================
def step4_fill_missing_info():
    """íˆ¬ìì ë° ì£¼ìš”ì‚¬ì—… ì •ë³´ ì±„ìš°ê¸°"""
    log(f"Step 4: ëˆ„ë½ ì •ë³´ ì±„ìš°ê¸°")

    # ì •ë³´ê°€ ë¶€ì¡±í•œ Deal ê°€ì ¸ì˜¤ê¸°
    deals = supabase.table('deals').select('*').or_(
        'investors.is.null,industry.is.null,industry.eq.-,investment_reason.is.null'
    ).execute()

    if not deals.data:
        log(f"  âœ… ëˆ„ë½ ì •ë³´ ì—†ìŒ")
        return

    log(f"  ğŸ“Š ì •ë³´ ë¶€ì¡±í•œ Deal: {len(deals.data)}ê°œ")

    updated = 0

    for deal in deals.data:
        company = deal['company_name']
        news_url = deal.get('news_url')
        news_title = deal.get('news_title', '')

        if not news_url:
            log(f"    âš ï¸ {company}: URL ì—†ìŒ")
            continue

        log(f"    ğŸ” {company}...")

        # ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§
        try:
            response = requests.get(news_url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                paragraphs = soup.find_all('p')
                content = ' '.join([p.get_text(strip=True) for p in paragraphs[:15]])
            else:
                content = ""
        except:
            content = ""

        # Geminië¡œ ì •ë³´ ì¶”ì¶œ
        prompt = f"""
ë‹¤ìŒ íˆ¬ììœ ì¹˜ ë‰´ìŠ¤ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:

ì œëª©: {news_title}
ë³¸ë¬¸: {content[:2000]}

JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€:
{{
    "investors": "íˆ¬ììëª… (ì½¤ë§ˆ êµ¬ë¶„, ì—†ìœ¼ë©´ null)",
    "industry": "ì£¼ìš”ì‚¬ì—… (2-4ë‹¨ì–´, ì—†ìœ¼ë©´ null)",
    "investment_reason": "ì´ íšŒì‚¬ê°€ íˆ¬ìë¥¼ ë°›ì€ í•µì‹¬ ì´ìœ  (ê¸°ìˆ ë ¥/ì‹œì¥ì„±/ë§¤ì¶œì„±ì¥ ë“± 1ë¬¸ì¥, ì—†ìœ¼ë©´ null)"
}}
"""

        try:
            gemini_response = gemini_client.models.generate_content(
                model='gemini-2.0-flash',
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0,
                    max_output_tokens=256,
                    response_mime_type='application/json'
                )
            )

            if gemini_response and hasattr(gemini_response, 'text'):
                info = json.loads(gemini_response.text.strip())
                if isinstance(info, list):
                    info = info[0] if info else {}

                update_data = {}

                if info.get('investors') and not deal.get('investors'):
                    update_data['investors'] = info['investors']

                if info.get('industry') and (not deal.get('industry') or deal.get('industry') == '-'):
                    update_data['industry'] = info['industry']
                    update_data['industry_category'] = categorize_industry(info['industry'])

                if info.get('investment_reason') and info['investment_reason'] != 'null':
                    update_data['investment_reason'] = info['investment_reason']

                if update_data:
                    supabase.table('deals').update(update_data).eq('id', deal['id']).execute()
                    log(f"      âœ… ì—…ë°ì´íŠ¸: {update_data}")
                    updated += 1
                else:
                    log(f"      âš ï¸ ì¶”ê°€ ì •ë³´ ì—†ìŒ")

        except Exception as e:
            log(f"      âŒ ì˜¤ë¥˜: {str(e)[:40]}", "ERROR")

        time.sleep(1)

    log(f"  âœ… {updated}ê°œ ì •ë³´ ì±„ì›€")


# ============================================================
# Step 5: ë„¤ì´ë²„ ë‰´ìŠ¤ â†’ ì‹¤ì œ ì–¸ë¡ ì‚¬ ë³€í™˜
# ============================================================
def step5_fix_naver_news():
    """ë„¤ì´ë²„ ë‰´ìŠ¤ë¡œ í‘œì‹œëœ í•­ëª©ì˜ ì‹¤ì œ ì–¸ë¡ ì‚¬ ì¶”ì¶œ"""
    log(f"Step 5: ë„¤ì´ë²„ ë‰´ìŠ¤ ì–¸ë¡ ì‚¬ ë³€í™˜")

    result = supabase.table('deals').select('id,company_name,site_name,news_url').eq('site_name', 'ë„¤ì´ë²„ ë‰´ìŠ¤').execute()

    if not result.data:
        log(f"  âœ… ë³€í™˜ í•„ìš” ì—†ìŒ")
        return

    log(f"  ğŸ“Š {len(result.data)}ê°œ í•­ëª© ì²˜ë¦¬ ì¤‘...")

    updated = 0
    for deal in result.data:
        real_site = extract_site_name_from_url(deal['news_url'])

        if real_site:
            supabase.table('deals').update({'site_name': real_site}).eq('id', deal['id']).execute()
            updated += 1

        time.sleep(0.5)

    log(f"  âœ… {updated}ê°œ ë³€í™˜ ì™„ë£Œ")


# ============================================================
# Step 5.5: ë„¤ì´ë²„ APIë¡œ ë°ì´í„° ì •ì œ/ê²€ì¦
# ============================================================
def step5_5_naver_api_enrichment():
    """ë„¤ì´ë²„ ë‰´ìŠ¤ APIë¡œ ë°ì´í„° ì •ì œ ë° ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘"""
    log(f"Step 5.5: ë„¤ì´ë²„ API ë°ì´í„° ì •ì œ")

    # ë„¤ì´ë²„ API í‚¤ í™•ì¸
    naver_client_id = os.getenv("NAVER_CLIENT_ID")
    naver_client_secret = os.getenv("NAVER_CLIENT_SECRET")

    if not naver_client_id or not naver_client_secret:
        log(f"  âš ï¸ ë„¤ì´ë²„ API í‚¤ ë¯¸ì„¤ì • (NAVER_CLIENT_ID, NAVER_CLIENT_SECRET)")
        log(f"  âš ï¸ https://developers.naver.com ì—ì„œ API í‚¤ ë°œê¸‰ í•„ìš”")
        return

    # íˆ¬ìì ì •ë³´ê°€ ì—†ëŠ” Deal ê°€ì ¸ì˜¤ê¸°
    deals_to_enrich = supabase.table('deals').select('*').or_(
        'investors.is.null,industry.is.null,industry.eq.-'
    ).execute()

    if not deals_to_enrich.data:
        log(f"  âœ… ì •ì œ í•„ìš” ì—†ìŒ")
        return

    log(f"  ğŸ“Š {len(deals_to_enrich.data)}ê°œ Deal ì •ì œ ì¤‘...")

    enriched = 0

    for deal in deals_to_enrich.data:
        company = deal['company_name']
        log(f"    ğŸ” {company}...")

        # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰
        search_query = f"{company} íˆ¬ì"
        url = f"https://openapi.naver.com/v1/search/news.json?query={quote(search_query)}&display=5&sort=date"

        headers = {
            "X-Naver-Client-Id": naver_client_id,
            "X-Naver-Client-Secret": naver_client_secret
        }

        try:
            response = requests.get(url, headers=headers, timeout=10)

            if response.status_code == 200:
                data = response.json()
                items = data.get('items', [])

                if items:
                    # ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ê¸°ì‚¬ì—ì„œ ì •ë³´ ì¶”ì¶œ
                    best_item = items[0]
                    title = best_item.get('title', '').replace('<b>', '').replace('</b>', '')
                    description = best_item.get('description', '').replace('<b>', '').replace('</b>', '')

                    # Geminië¡œ ì •ë³´ ì¶”ì¶œ
                    combined_text = f"ì œëª©: {title}\në‚´ìš©: {description}"

                    extract_prompt = f"""
ë‹¤ìŒ ë‰´ìŠ¤ì—ì„œ íˆ¬ì ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:
{combined_text}

JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€:
{{
    "investors": "íˆ¬ììëª… (ì—†ìœ¼ë©´ null)",
    "industry": "ì£¼ìš”ì‚¬ì—… (2-3ë‹¨ì–´, ì—†ìœ¼ë©´ null)",
    "amount": "íˆ¬ìê¸ˆì•¡ (ì–µì› ìˆ«ìë§Œ, ì—†ìœ¼ë©´ null)"
}}
"""
                    extract_response = gemini_client.models.generate_content(
                        model='gemini-2.5-flash',
                        contents=extract_prompt,
                        config=types.GenerateContentConfig(
                            temperature=0,
                            max_output_tokens=256,
                            response_mime_type='application/json'
                        )
                    )

                    if extract_response and hasattr(extract_response, 'text'):
                        info = json.loads(extract_response.text.strip())
                        if isinstance(info, list):
                            info = info[0] if info else {}

                        # ì—…ë°ì´íŠ¸í•  í•„ë“œ ê²°ì •
                        update_data = {}

                        if info.get('investors') and not deal.get('investors'):
                            update_data['investors'] = info['investors']

                        if info.get('industry') and (not deal.get('industry') or deal.get('industry') == '-'):
                            update_data['industry'] = info['industry']
                            update_data['industry_category'] = categorize_industry(info['industry'])

                        if info.get('amount') and not deal.get('amount'):
                            update_data['amount'] = info['amount']

                        if update_data:
                            supabase.table('deals').update(update_data).eq('id', deal['id']).execute()
                            log(f"      âœ… ì—…ë°ì´íŠ¸: {list(update_data.keys())}")
                            enriched += 1
                        else:
                            log(f"      âš ï¸ ì¶”ê°€ ì •ë³´ ì—†ìŒ")

                else:
                    log(f"      âš ï¸ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")

            else:
                log(f"      âŒ API ì˜¤ë¥˜: {response.status_code}", "ERROR")

        except Exception as e:
            log(f"      âŒ ì˜¤ë¥˜: {str(e)[:40]}", "ERROR")

        time.sleep(0.5)  # API ì œí•œ ê³ ë ¤

    log(f"  âœ… {enriched}ê°œ ì •ì œ ì™„ë£Œ")


# ============================================================
# Step 6: Deal ë²ˆí˜¸ ì¬ì •ë ¬
# ============================================================
def step6_renumber_deals():
    """Deal ë²ˆí˜¸ë¥¼ ìµœì‹ ìˆœìœ¼ë¡œ ì¬ì •ë ¬"""
    log(f"Step 6: Deal ë²ˆí˜¸ ì¬ì •ë ¬")

    deals = supabase.table('deals').select('*').order('news_date', desc=True).order('id', desc=True).execute()

    log(f"  ğŸ“Š ì´ {len(deals.data)}ê°œ Deal ì¬ì •ë ¬ ì¤‘...")

    # Step 1: ìŒìˆ˜ë¡œ ë³€ê²½ (ì¤‘ë³µ ë°©ì§€)
    for i, deal in enumerate(deals.data, 1):
        supabase.table('deals').update({'number': -i}).eq('id', deal['id']).execute()

    # Step 2: ì–‘ìˆ˜ë¡œ ë³€ê²½
    for new_number, deal in enumerate(deals.data, 1):
        supabase.table('deals').update({'number': new_number}).eq('id', deal['id']).execute()

    log(f"  âœ… ìµœì‹ ìˆœ 1~{len(deals.data)}ë²ˆ ì¬ì •ë ¬ ì™„ë£Œ")


# ============================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================
def main():
    parser = argparse.ArgumentParser(description='ë§¤ì¼ ìë™ ë‰´ìŠ¤ ìˆ˜ì§‘')
    parser.add_argument('--date', type=str, help='ìˆ˜ì§‘ ëŒ€ìƒ ë‚ ì§œ (YYYY-MM-DD)', default=None)
    args = parser.parse_args()

    # ëŒ€ìƒ ë‚ ì§œ ê²°ì • (KST ê¸°ì¤€)
    KST = timezone(timedelta(hours=9))
    if args.date:
        target_date = args.date
    else:
        # ê¸°ë³¸: KST ê¸°ì¤€ ì–´ì œ (GitHub ActionsëŠ” UTCì´ë¯€ë¡œ KST ë³€í™˜ í•„ìˆ˜)
        target_date = (datetime.now(KST) - timedelta(days=1)).strftime('%Y-%m-%d')

    print("=" * 70)
    print("ğŸ“° ë§¤ì¼ ìë™ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
    print(f"â° ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ¯ ìˆ˜ì§‘ ëŒ€ìƒ ë‚ ì§œ: {target_date}")
    print("=" * 70)

    try:
        # Step 1: ì›¹ í¬ë¡¤ë§
        articles = step1_crawl_media_sites(target_date)

        # Step 1.5: Google Search ì¶”ê°€ ìˆ˜ì§‘
        existing_urls = {a['url'] for a in articles}
        google_articles = step1_5_google_search(target_date, existing_urls)
        articles.extend(google_articles)

        if articles:
            # Step 2: ê²€ì¦ ë° ì €ì¥
            saved = step2_verify_and_save(articles, target_date)

            # Step 3: Deal ë“±ë¡ (ì €ì¥ëœ ë‰´ìŠ¤ ì—†ì–´ë„ ì‹œë„ - ì´ë¯¸ ì €ì¥ëœ ë‰´ìŠ¤ê°€ ìˆì„ ìˆ˜ ìˆìŒ)
            registered = step3_register_to_deals(target_date)

            # Step 4: ëˆ„ë½ ì •ë³´ ì±„ìš°ê¸°
            step4_fill_missing_info()

            # Step 5: ë„¤ì´ë²„ ë‰´ìŠ¤ ë³€í™˜
            step5_fix_naver_news()

            # Step 5.5: ë„¤ì´ë²„ APIë¡œ ë°ì´í„° ì •ì œ
            step5_5_naver_api_enrichment()

            # Step 6: ë²ˆí˜¸ ì¬ì •ë ¬
            step6_renumber_deals()

        print("\n" + "=" * 70)
        print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        print("=" * 70)

    except Exception as e:
        log(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "ERROR")
        raise


if __name__ == '__main__':
    main()
